# 3장 신경망

신경망은 입력층과 은닉층, 출력층이 존재함

x1, x2라는 값이 주어지고 가중치가 w1, w2일 때 출력은

$$
y = \begin{cases} 
  0 (b+W_1X_1 + W_2X_2 \leq 0)\\
  1 (b+W_1X_1 + W_2X_2 > 0)
\end{cases}
$$

아래 수식처럼 표현 가능하다

여기서 b는 편향을 나타내는 매개변수로, 뉴런이 얼마나 쉽게 활성화되는냐를 제어한다.

편향을 명시한 퍼셉트론에서는 가중치가 b이고 입력이 1인 뉴런이 추가된다.

함수 h(x)를 표현

$$
y = h(b+ W_1X_1 + W_2X_2)\\
h(x) = y = \begin{cases} 
  0 (X\leq 0)\\
  1 (X > 0)
\end{cases}
$$

입력 신호의 총합이 h(x)라는 함수를 거쳐 변환되어, 그 변환된 값이 y의 출력이 됨을 보여준다. 입력이 0을 넘으면 1을 반환

그렇지 않으면 0을 반환한다

입력 신호의 총합을 출력신호로 변환하는 함수 → 활성화 함수

활성화 함수는 입력 신호 총합이 활성화를 일으키는지를 정하는 역할을 합니다.

$$
h(x) = {1 \over 1 + exp(-x)}
$$

활성화 함수는 임계값을 경계로 출력이 바뀌는데, 이런 함수를 계단 함수라고 한다.

계단 함수 그래프

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/eac6d6c1-5a54-4da0-8b73-664e695b6ca1/Untitled.png)

## 시그모이드 함수

시그모이드 함수의 수식이다. exp(-x)는 e^-x을 의미한다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7cbe4fda-6589-4cbc-93fe-27e5a57af140/Untitled.png)

### 시그모이드 함수와 계단 함수의 차이점

매끄러움의 차이와 출력 값의 차이

매끄러움은 신경항 학습에서 중요한 부분임 / 출력값의 차이는 계단 함수는 0또는 1의 출력 값만 가지지만, 시그모이드 함수는 0.8123, 0.3 등 다양한 실수 값이 출력될 수 있음

두 함수는 비선형 함수인데 이 뜻은 직선 1개로 그릴 수 없는 그래프를 말하는 것

신경망에서는 활성화 함수로는 비선형 함수로 사용해야한다. 

선형 함수인 h(x) = cx를 활성화 함수로 사용한 3층 네트워크는 수식적으로 y(x) = h(h(h(x)))가 된다. 이 계산은 y(x) = c * c * c * x처럼 곱셈을 3번 하지만

y(x) = ax( a= c^3)인 식과 동일하다. 따라서 선형 함수를 사용하게되면, 은닉층이 없는 네트워크로 표현할 수 있다. 

## ReLU 함수

입력이 0이하면 모두 0을 반환, 0 초과이면 값을 그대로 반환
