신경망은 입력층과 은닉층, 출력층이 존재함
x1, x2라는 값이 주어지고 가중치가 w1, w2일 때 출력은

$$
y = \begin{cases} 
  0 (b+W_1X_1 + W_2X_2 \leq 0)\\
  1 (b+W_1X_1 + W_2X_2 > 0)
\end{cases}
$$

여기서 b는 편향, 뉴런이 얼마나 쉽게 활성화되는가 / 편향을 명시한 퍼셉트론에서는 가중치가 b이고 입력이 1인 뉴런이 추가된다.
함수 h(x)로 표현
$$
y = h(b+ W_1X_1 + W_2X_2)\\
h(x) = y = \begin{cases} 
  0 (X\leq 0)\\
  1 (X > 0)
\end{cases}
$$

입력이 0 초과 -> 1 출력 / 0이하 -> 0 출력
활성화 함수 : 입력 신호의 총합을 출력신호로 변환하는 함수(활성화를 하느냐 안하느냐)

활성화 함수는 임계값을 경계로 출력이 바뀌는데, 이런 함수를 계단 함수라고 한다.
계단 함수 그래프
![Untitled](./img/stepfunction.png)

## 시그모이드 함수
$$
h(x) = {1 \over 1 + exp(-x)}
$$
exp(-x)는 e^-x을 의미한다.
![Untitled](./img/sigmpidfunction.png)

### 시그모이드 함수와 계단 함수의 차이점
매끄러움의 차이와 출력 값의 차이
비선형 함수를 사용하는 이유는 선형 함수로는 복잡한 네트워크를 표현할 수 없음
매끄러움(곡선)은 추후에 기울기에 영향을 준다.

## ReLU 함수
입력이 0이하면 모두 0을 반환, 0 초과이면 값을 그대로 반환
수식으로 표현하면 이런 식으로 표현 가능
$$
h(x) = \begin{cases} 
  0 (x\leq 0)\\
  1 (x > 0)
\end{cases}
$$
![Untitled](./img/ReLUGraph.png)
--------------------------------------------------------------------------------
# 출력층 설계
회귀에는 항등 함수,분류에는 소프트맥스 함수 사용
항등 함수 : 입력은 그대로 반환

### 소프트 맥스 함수의 수식
$$
y_k = {exp(a_k) \over \overset{n}{ \underset{i=1}{\sum}}exp(a_i)}
$$
n은 출력층의 뉴런 수, Yk는 그 중 k번째 출력, 큰 수 입력 -> 오버플로우

오버플로우 문제를 해결한 수식
$$
y_k = {exp(a_k + C') \over \overset{n}{ \underset{i=1}{\sum}}exp(a_i + C')}(C'=logC)
$$
소프트맥스 함수의 출력값은 0~1 사이의 소수이며, 출력값의 총합은 1(확률적 해석 가능)

## 손글씨 숫자 인식
Mnist 데이터 셋을 이용
load_mnist 함수의 인수는 normalize, flatten, one_hot_label
normalize : 입력 이미지의 픽셀 값을 0.0 ~ 1.0 사이로 정규화, False -> 디폴트인 0 ~ 255 사이의 값
flatten : 입력 이미지 데이터를 평탄하게(1차원 형태로) 만듦
one_hot_label : one-hot-encoding(정답인 데이터만 1, 나머지는 0인 형태) 형태로 저장할지를 설정

### 배치처리
묶어서 처리하는 것

배치의 장점은 이미지 1장당 처리 시간을 줄여준다. 이유는 2가지
1. 수치 계산 라이브러리는 큰 배열을 빠르게 처리할 수 있도록 발달 되어있음.
2. 버스(데이터가 이동하는 통로)에 주는 부하를 줄인다.